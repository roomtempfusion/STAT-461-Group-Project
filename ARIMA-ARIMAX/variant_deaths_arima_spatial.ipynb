{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Modeling for Variant-Specific Death Predictions with Spatial Features\n",
    "\n",
    "This notebook implements ARIMA/ARIMAX models to predict COVID-19 deaths by variant, incorporating spatial connectivity through adjacency matrices.\n",
    "\n",
    "## Approach:\n",
    "1. Calculate variant-specific deaths using variant prevalence proportions\n",
    "2. Use different time windows for each variant based on their active periods\n",
    "3. Incorporate spatial features from adjacency matrices (border, airport, highway)\n",
    "4. Build separate ARIMA/ARIMAX models for each variant\n",
    "5. Evaluate and compare predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Adjacency Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the main dataset\ndf = pd.read_csv('../processed data/combined_prevalence_and_exogenous.csv')\ndf['date'] = pd.to_datetime(df['date'])\n\n# Load adjacency matrices and ensure both index and columns are integers\nborder_adj = pd.read_csv('../processed data/border_adj_matrix.csv', index_col=0)\nborder_adj.columns = border_adj.columns.astype(int)\n\nairport_adj = pd.read_csv('../processed data/airport_adj_matrix.csv', index_col=0)\nairport_adj.columns = airport_adj.columns.astype(int)\n\nhighway_adj = pd.read_csv('../processed data/highway_adj_matrix.csv', index_col=0)\nhighway_adj.columns = highway_adj.columns.astype(int)\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\nprint(f\"Counties: {df['location'].nunique()}\")\nprint(f\"\\nAdjacency matrix shape: {border_adj.shape}\")\nprint(f\"Adjacency index dtype: {border_adj.index.dtype}\")\nprint(f\"Adjacency columns dtype: {border_adj.columns.dtype}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Variant Time Windows\n",
    "\n",
    "Based on prevalence analysis, we use these time windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant Time Windows for Modeling:\n",
      "============================================================\n",
      "Alpha     : 2021-01-17 to 2021-08-18 (213 days)\n",
      "Delta     : 2021-01-10 to 2022-02-18 (404 days)\n",
      "Epsilon   : 2020-10-27 to 2021-05-25 (210 days)\n",
      "Iota      : 2021-02-10 to 2021-07-10 (150 days)\n"
     ]
    }
   ],
   "source": [
    "# Define time windows for each variant (when they were actively spreading)\n",
    "variant_windows = {\n",
    "    'Alpha': ('2021-01-17', '2021-08-18'),\n",
    "    'Delta': ('2021-01-10', '2022-02-18'),\n",
    "    'Epsilon': ('2020-10-27', '2021-05-25'),\n",
    "    'Iota': ('2021-02-10', '2021-07-10')\n",
    "}\n",
    "\n",
    "# Display variant windows\n",
    "print(\"Variant Time Windows for Modeling:\")\n",
    "print(\"=\"*60)\n",
    "for variant, (start, end) in variant_windows.items():\n",
    "    days = (pd.to_datetime(end) - pd.to_datetime(start)).days\n",
    "    print(f\"{variant:10s}: {start} to {end} ({days} days)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate Variant-Specific Deaths\n",
    "\n",
    "Deaths attributed to each variant = Total deaths Ã— Variant prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant-specific death columns created:\n",
      "['Alpha_deaths', 'Delta_deaths', 'Epsilon_deaths', 'Iota_deaths']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>Alpha_deaths</th>\n",
       "      <th>Delta_deaths</th>\n",
       "      <th>Epsilon_deaths</th>\n",
       "      <th>Iota_deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8182</th>\n",
       "      <td>2021-04-22</td>\n",
       "      <td>17001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8183</th>\n",
       "      <td>2021-04-23</td>\n",
       "      <td>17001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184</th>\n",
       "      <td>2021-04-25</td>\n",
       "      <td>17001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8185</th>\n",
       "      <td>2021-04-26</td>\n",
       "      <td>17001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8186</th>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>17001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>2021-05-04</td>\n",
       "      <td>17001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>2021-05-05</td>\n",
       "      <td>17001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8189</th>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>17001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8190</th>\n",
       "      <td>2021-05-10</td>\n",
       "      <td>17001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8191</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>17001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  location  new_deaths  Alpha_deaths  Delta_deaths  \\\n",
       "8182 2021-04-22     17001         0.0      0.000000           0.0   \n",
       "8183 2021-04-23     17001         0.0      0.000000           0.0   \n",
       "8184 2021-04-25     17001         2.0      0.857143           0.0   \n",
       "8185 2021-04-26     17001         0.0      0.000000           0.0   \n",
       "8186 2021-04-27     17001         0.0      0.000000           0.0   \n",
       "8187 2021-05-04     17001         1.0      0.000000           0.0   \n",
       "8188 2021-05-05     17001         1.0      0.500000           0.0   \n",
       "8189 2021-05-06     17001         1.0      0.750000           0.0   \n",
       "8190 2021-05-10     17001         1.0      0.833333           0.0   \n",
       "8191 2021-05-11     17001         0.0      0.000000           0.0   \n",
       "\n",
       "      Epsilon_deaths  Iota_deaths  \n",
       "8182             0.0          0.0  \n",
       "8183             0.0          0.0  \n",
       "8184             0.0          0.0  \n",
       "8185             0.0          0.0  \n",
       "8186             0.0          0.0  \n",
       "8187             0.0          0.0  \n",
       "8188             0.0          0.0  \n",
       "8189             0.0          0.0  \n",
       "8190             0.0          0.0  \n",
       "8191             0.0          0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate daily new deaths\n",
    "df = df.sort_values(['location', 'date'])\n",
    "df['new_deaths'] = df.groupby('location')['deaths'].diff().clip(lower=0).fillna(0)\n",
    "\n",
    "# Calculate variant-specific deaths\n",
    "variants = ['Alpha', 'Delta', 'Epsilon', 'Iota']\n",
    "\n",
    "for variant in variants:\n",
    "    df[f'{variant}_deaths'] = df['new_deaths'] * df[variant]\n",
    "\n",
    "print(\"Variant-specific death columns created:\")\n",
    "print([f'{v}_deaths' for v in variants])\n",
    "\n",
    "# Display sample\n",
    "df[['date', 'location', 'new_deaths'] + [f'{v}_deaths' for v in variants]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Spatial Features from Adjacency Matrices\n",
    "\n",
    "For each county, we calculate weighted neighbor deaths using adjacency matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def calculate_spatial_lag(df, date_col, county_col, value_col, adjacency_matrix, normalize=True):\n    \"\"\"\n    Calculate spatial lag (weighted neighbor values) for each county.\n    \n    Parameters:\n    - df: DataFrame with time series data\n    - date_col: name of date column\n    - county_col: name of county identifier column\n    - value_col: name of value column to calculate spatial lag for\n    - adjacency_matrix: DataFrame with adjacency weights\n    - normalize: whether to normalize by row sum\n    \n    Returns:\n    - Series with spatial lag values\n    \"\"\"\n    # Normalize adjacency matrix if requested\n    adj = adjacency_matrix.copy()\n    if normalize:\n        row_sums = adj.sum(axis=1)\n        row_sums[row_sums == 0] = 1  # Avoid division by zero\n        adj = adj.div(row_sums, axis=0)\n    \n    spatial_lags = []\n    \n    for date in df[date_col].unique():\n        date_data = df[df[date_col] == date].set_index(county_col)[value_col]\n        \n        # Ensure counties are aligned (both are int64, no conversion needed)\n        date_data = date_data.reindex(adj.index, fill_value=0)\n        \n        # Calculate spatial lag: W Ã— y\n        lag = adj.dot(date_data)\n        \n        # Create result for this date\n        for county in df[df[date_col] == date][county_col]:\n            if county in lag.index:\n                spatial_lags.append(lag[county])\n            else:\n                spatial_lags.append(0)\n    \n    return pd.Series(spatial_lags, index=df.index)\n\nprint(\"Calculating spatial lag features...\")\n\n# Note: Both df['location'] and adjacency matrix indices are int64 - no conversion needed\n\n# Calculate spatial lags for each variant's deaths\nfor variant in variants:\n    print(f\"  Processing {variant}...\")\n    death_col = f'{variant}_deaths'\n    \n    # Calculate spatial lags using different adjacency matrices\n    df[f'{variant}_deaths_border_lag'] = calculate_spatial_lag(\n        df, 'date', 'location', death_col, border_adj, normalize=True\n    )\n    \n    df[f'{variant}_deaths_airport_lag'] = calculate_spatial_lag(\n        df, 'date', 'location', death_col, airport_adj, normalize=True\n    )\n    \n    df[f'{variant}_deaths_highway_lag'] = calculate_spatial_lag(\n        df, 'date', 'location', death_col, highway_adj, normalize=True\n    )\n\nprint(\"\\nSpatial lag features created!\")\nprint(f\"New columns: {[col for col in df.columns if '_lag' in col][:3]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregate Data to State Level for Modeling\n",
    "\n",
    "We'll model at the state level first, then can extend to county-level if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to state level (sum across all counties)\n",
    "state_data = df.groupby('date').agg({\n",
    "    'new_deaths': 'sum',\n",
    "    **{f'{v}_deaths': 'sum' for v in variants},\n",
    "    **{f'{v}_deaths_border_lag': 'sum' for v in variants},\n",
    "    **{f'{v}_deaths_airport_lag': 'sum' for v in variants},\n",
    "    **{f'{v}_deaths_highway_lag': 'sum' for v in variants},\n",
    "}).reset_index().sort_values('date')\n",
    "\n",
    "print(f\"State-level aggregated data shape: {state_data.shape}\")\n",
    "state_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Variant-Specific Deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, variant in enumerate(variants):\n",
    "    # Filter to variant's active period\n",
    "    start, end = variant_windows[variant]\n",
    "    variant_data = state_data[(state_data['date'] >= start) & (state_data['date'] <= end)]\n",
    "    \n",
    "    death_col = f'{variant}_deaths'\n",
    "    \n",
    "    # Calculate 7-day rolling average\n",
    "    variant_data_plot = variant_data.copy()\n",
    "    variant_data_plot['deaths_7d'] = variant_data_plot[death_col].rolling(7, min_periods=1).mean()\n",
    "    \n",
    "    axes[idx].plot(variant_data_plot['date'], variant_data_plot[death_col], \n",
    "                   alpha=0.3, label='Daily', color='lightblue')\n",
    "    axes[idx].plot(variant_data_plot['date'], variant_data_plot['deaths_7d'], \n",
    "                   label='7-day avg', color='darkblue', linewidth=2)\n",
    "    axes[idx].set_title(f'{variant} Variant Deaths', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date')\n",
    "    axes[idx].set_ylabel('Daily Deaths')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stationarity Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series, name=''):\n",
    "    \"\"\"Augmented Dickey-Fuller test for stationarity\"\"\"\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'{name} ADF Test:')\n",
    "    print(f'  ADF Statistic: {result[0]:.4f}')\n",
    "    print(f'  p-value: {result[1]:.4f}')\n",
    "    print(f'  Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'    {key}: {value:.4f}')\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(f'  Result: Series is STATIONARY (reject H0)\\n')\n",
    "    else:\n",
    "        print(f'  Result: Series is NON-STATIONARY (fail to reject H0)\\n')\n",
    "    \n",
    "    return result[1] <= 0.05\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STATIONARITY TESTS FOR VARIANT DEATHS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for variant in variants:\n",
    "    start, end = variant_windows[variant]\n",
    "    variant_data = state_data[(state_data['date'] >= start) & (state_data['date'] <= end)]\n",
    "    \n",
    "    death_col = f'{variant}_deaths'\n",
    "    series = variant_data[death_col]\n",
    "    \n",
    "    adf_test(series, f'{variant} Deaths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ACF and PACF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2, figsize=(16, 16))\n",
    "\n",
    "for idx, variant in enumerate(variants):\n",
    "    start, end = variant_windows[variant]\n",
    "    variant_data = state_data[(state_data['date'] >= start) & (state_data['date'] <= end)]\n",
    "    \n",
    "    death_col = f'{variant}_deaths'\n",
    "    series = variant_data[death_col].dropna()\n",
    "    \n",
    "    # ACF\n",
    "    plot_acf(series, lags=min(40, len(series)//2), ax=axes[idx, 0])\n",
    "    axes[idx, 0].set_title(f'{variant} - Autocorrelation Function', fontweight='bold')\n",
    "    \n",
    "    # PACF\n",
    "    plot_pacf(series, lags=min(40, len(series)//2), ax=axes[idx, 1])\n",
    "    axes[idx, 1].set_title(f'{variant} - Partial Autocorrelation Function', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build ARIMA Models for Each Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(actual, predicted):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if len(actual) > 0 and actual.var() > 0:\n",
    "        r2 = r2_score(actual, predicted)\n",
    "    else:\n",
    "        r2 = np.nan\n",
    "    \n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "def train_arima_model(data, target_col, order=(1,1,1), train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Train ARIMA model on time series data\n",
    "    \n",
    "    Returns:\n",
    "    - model: fitted ARIMA model\n",
    "    - train_data, test_data: train/test splits\n",
    "    - predictions: forecasts on test set\n",
    "    - metrics: evaluation metrics\n",
    "    \"\"\"\n",
    "    # Sort by date\n",
    "    data = data.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Split into train/test\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    train = data[:train_size]\n",
    "    test = data[train_size:]\n",
    "    \n",
    "    # Prepare series\n",
    "    train_series = train[target_col]\n",
    "    test_series = test[target_col]\n",
    "    \n",
    "    # Fit ARIMA model\n",
    "    model = ARIMA(train_series, order=order)\n",
    "    fitted_model = model.fit()\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = fitted_model.forecast(steps=len(test))\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(test_series.values, predictions)\n",
    "    \n",
    "    return fitted_model, train, test, predictions, metrics\n",
    "\n",
    "# Store results\n",
    "arima_results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ARIMA MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for variant in variants:\n",
    "    print(f\"\\n{variant} Variant:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Filter data for this variant's time window\n",
    "    start, end = variant_windows[variant]\n",
    "    variant_data = state_data[(state_data['date'] >= start) & (state_data['date'] <= end)].copy()\n",
    "    \n",
    "    if len(variant_data) < 20:\n",
    "        print(f\"  Insufficient data ({len(variant_data)} observations)\")\n",
    "        continue\n",
    "    \n",
    "    death_col = f'{variant}_deaths'\n",
    "    \n",
    "    # Try different ARIMA orders\n",
    "    orders_to_try = [(1,1,1), (2,1,2), (1,0,1), (2,0,2), (3,1,3)]\n",
    "    best_aic = np.inf\n",
    "    best_model = None\n",
    "    best_order = None\n",
    "    \n",
    "    for order in orders_to_try:\n",
    "        try:\n",
    "            model, train, test, preds, metrics = train_arima_model(\n",
    "                variant_data, death_col, order=order, train_ratio=0.8\n",
    "            )\n",
    "            \n",
    "            if model.aic < best_aic:\n",
    "                best_aic = model.aic\n",
    "                best_model = model\n",
    "                best_order = order\n",
    "                best_train = train\n",
    "                best_test = test\n",
    "                best_preds = preds\n",
    "                best_metrics = metrics\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if best_model is not None:\n",
    "        print(f\"  Best ARIMA order: {best_order}\")\n",
    "        print(f\"  AIC: {best_aic:.2f}\")\n",
    "        print(f\"  Train size: {len(best_train)}\")\n",
    "        print(f\"  Test size: {len(best_test)}\")\n",
    "        print(f\"  \\nTest Set Metrics:\")\n",
    "        for metric, value in best_metrics.items():\n",
    "            print(f\"    {metric}: {value:.4f}\")\n",
    "        \n",
    "        arima_results[variant] = {\n",
    "            'model': best_model,\n",
    "            'order': best_order,\n",
    "            'train': best_train,\n",
    "            'test': best_test,\n",
    "            'predictions': best_preds,\n",
    "            'metrics': best_metrics\n",
    "        }\n",
    "    else:\n",
    "        print(f\"  Failed to fit model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Build ARIMAX Models with Spatial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_arimax_model(data, target_col, exog_cols, order=(1,1,1), train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Train ARIMAX model with exogenous variables\n",
    "    \"\"\"\n",
    "    data = data.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    train = data[:train_size]\n",
    "    test = data[train_size:]\n",
    "    \n",
    "    train_series = train[target_col]\n",
    "    test_series = test[target_col]\n",
    "    train_exog = train[exog_cols]\n",
    "    test_exog = test[exog_cols]\n",
    "    \n",
    "    # Fit SARIMAX (ARIMAX)\n",
    "    model = SARIMAX(train_series, exog=train_exog, order=order)\n",
    "    fitted_model = model.fit(disp=False)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = fitted_model.forecast(steps=len(test), exog=test_exog)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(test_series.values, predictions)\n",
    "    \n",
    "    return fitted_model, train, test, predictions, metrics\n",
    "\n",
    "arimax_results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ARIMAX MODELS WITH SPATIAL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for variant in variants:\n",
    "    print(f\"\\n{variant} Variant:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    start, end = variant_windows[variant]\n",
    "    variant_data = state_data[(state_data['date'] >= start) & (state_data['date'] <= end)].copy()\n",
    "    \n",
    "    if len(variant_data) < 20:\n",
    "        print(f\"  Insufficient data ({len(variant_data)} observations)\")\n",
    "        continue\n",
    "    \n",
    "    death_col = f'{variant}_deaths'\n",
    "    \n",
    "    # Spatial lag features as exogenous variables\n",
    "    exog_cols = [\n",
    "        f'{variant}_deaths_border_lag',\n",
    "        f'{variant}_deaths_airport_lag',\n",
    "        f'{variant}_deaths_highway_lag'\n",
    "    ]\n",
    "    \n",
    "    # Get best order from ARIMA results\n",
    "    if variant in arima_results:\n",
    "        best_order = arima_results[variant]['order']\n",
    "    else:\n",
    "        best_order = (1, 1, 1)\n",
    "    \n",
    "    try:\n",
    "        model, train, test, preds, metrics = train_arimax_model(\n",
    "            variant_data, death_col, exog_cols, order=best_order, train_ratio=0.8\n",
    "        )\n",
    "        \n",
    "        print(f\"  ARIMAX order: {best_order}\")\n",
    "        print(f\"  AIC: {model.aic:.2f}\")\n",
    "        print(f\"  Exogenous features: {len(exog_cols)}\")\n",
    "        print(f\"  Train size: {len(train)}\")\n",
    "        print(f\"  Test size: {len(test)}\")\n",
    "        print(f\"  \\nTest Set Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"    {metric}: {value:.4f}\")\n",
    "        \n",
    "        arimax_results[variant] = {\n",
    "            'model': model,\n",
    "            'order': best_order,\n",
    "            'train': train,\n",
    "            'test': test,\n",
    "            'predictions': preds,\n",
    "            'metrics': metrics,\n",
    "            'exog_cols': exog_cols\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to fit ARIMAX: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare ARIMA vs ARIMAX Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison = []\n",
    "\n",
    "for variant in variants:\n",
    "    if variant in arima_results and variant in arimax_results:\n",
    "        arima_metrics = arima_results[variant]['metrics']\n",
    "        arimax_metrics = arimax_results[variant]['metrics']\n",
    "        \n",
    "        comparison.append({\n",
    "            'Variant': variant,\n",
    "            'ARIMA_RMSE': arima_metrics['RMSE'],\n",
    "            'ARIMAX_RMSE': arimax_metrics['RMSE'],\n",
    "            'ARIMA_MAE': arima_metrics['MAE'],\n",
    "            'ARIMAX_MAE': arimax_metrics['MAE'],\n",
    "            'ARIMA_R2': arima_metrics['R2'],\n",
    "            'ARIMAX_R2': arimax_metrics['R2'],\n",
    "            'Improvement_RMSE': ((arima_metrics['RMSE'] - arimax_metrics['RMSE']) / arima_metrics['RMSE'] * 100)\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON: ARIMA vs ARIMAX\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "if len(comparison_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    metrics_to_plot = ['RMSE', 'MAE', 'R2']\n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        comparison_df.plot(x='Variant', y=[f'ARIMA_{metric}', f'ARIMAX_{metric}'], \n",
    "                          kind='bar', ax=axes[idx])\n",
    "        axes[idx].set_title(f'{metric} Comparison', fontweight='bold')\n",
    "        axes[idx].set_ylabel(metric)\n",
    "        axes[idx].legend(['ARIMA', 'ARIMAX'])\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, variant in enumerate(variants):\n",
    "    if variant not in arima_results:\n",
    "        continue\n",
    "    \n",
    "    # Get data\n",
    "    train = arima_results[variant]['train']\n",
    "    test = arima_results[variant]['test']\n",
    "    arima_preds = arima_results[variant]['predictions']\n",
    "    \n",
    "    death_col = f'{variant}_deaths'\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].plot(train['date'], train[death_col], label='Training Data', color='blue')\n",
    "    axes[idx].plot(test['date'], test[death_col], label='Actual Test Data', color='green', linewidth=2)\n",
    "    axes[idx].plot(test['date'], arima_preds, label='ARIMA Predictions', \n",
    "                   color='red', linestyle='--', linewidth=2)\n",
    "    \n",
    "    if variant in arimax_results:\n",
    "        arimax_preds = arimax_results[variant]['predictions']\n",
    "        axes[idx].plot(test['date'], arimax_preds, label='ARIMAX Predictions', \n",
    "                       color='orange', linestyle=':', linewidth=2)\n",
    "    \n",
    "    axes[idx].set_title(f'{variant} Variant - Death Predictions', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date')\n",
    "    axes[idx].set_ylabel('Daily Deaths')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(variants), 2, figsize=(16, 4*len(variants)))\n",
    "\n",
    "for idx, variant in enumerate(variants):\n",
    "    if variant not in arima_results:\n",
    "        continue\n",
    "    \n",
    "    # ARIMA residuals\n",
    "    model = arima_results[variant]['model']\n",
    "    residuals = model.resid\n",
    "    \n",
    "    # Plot residuals over time\n",
    "    axes[idx, 0].plot(residuals)\n",
    "    axes[idx, 0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[idx, 0].set_title(f'{variant} - ARIMA Residuals Over Time', fontweight='bold')\n",
    "    axes[idx, 0].set_ylabel('Residuals')\n",
    "    axes[idx, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals distribution\n",
    "    axes[idx, 1].hist(residuals, bins=30, edgecolor='black')\n",
    "    axes[idx, 1].set_title(f'{variant} - Residuals Distribution', fontweight='bold')\n",
    "    axes[idx, 1].set_xlabel('Residuals')\n",
    "    axes[idx, 1].set_ylabel('Frequency')\n",
    "    axes[idx, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. VARIANT TIME WINDOWS:\")\n",
    "for variant, (start, end) in variant_windows.items():\n",
    "    print(f\"   {variant}: {start} to {end}\")\n",
    "\n",
    "print(\"\\n2. MODELS TRAINED:\")\n",
    "print(f\"   ARIMA models: {len(arima_results)}\")\n",
    "print(f\"   ARIMAX models: {len(arimax_results)}\")\n",
    "\n",
    "print(\"\\n3. BEST PERFORMING VARIANT (by RÂ²):\")\n",
    "if len(comparison_df) > 0:\n",
    "    best_arima = comparison_df.nlargest(1, 'ARIMA_R2')[['Variant', 'ARIMA_R2']]\n",
    "    best_arimax = comparison_df.nlargest(1, 'ARIMAX_R2')[['Variant', 'ARIMAX_R2']]\n",
    "    print(f\"   ARIMA: {best_arima.iloc[0]['Variant']} (RÂ² = {best_arima.iloc[0]['ARIMA_R2']:.4f})\")\n",
    "    print(f\"   ARIMAX: {best_arimax.iloc[0]['Variant']} (RÂ² = {best_arimax.iloc[0]['ARIMAX_R2']:.4f})\")\n",
    "\n",
    "print(\"\\n4. SPATIAL FEATURES IMPACT:\")\n",
    "if len(comparison_df) > 0:\n",
    "    avg_improvement = comparison_df['Improvement_RMSE'].mean()\n",
    "    print(f\"   Average RMSE improvement with spatial features: {avg_improvement:.2f}%\")\n",
    "\n",
    "print(\"\\n5. KEY FINDINGS:\")\n",
    "print(\"   - Different variants showed distinct temporal patterns\")\n",
    "print(\"   - Delta variant had the longest active period (397 days)\")\n",
    "print(\"   - Spatial connectivity features from adjacency matrices were incorporated\")\n",
    "print(\"   - ARIMAX models account for neighbor county spillover effects\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "if len(comparison_df) > 0:\n",
    "    comparison_df.to_csv('arima_arimax_comparison.csv', index=False)\n",
    "    print(\"Comparison results saved to 'arima_arimax_comparison.csv'\")\n",
    "\n",
    "# Save predictions for each variant\n",
    "for variant in variants:\n",
    "    if variant in arima_results:\n",
    "        test = arima_results[variant]['test'].copy()\n",
    "        test['arima_predictions'] = arima_results[variant]['predictions']\n",
    "        \n",
    "        if variant in arimax_results:\n",
    "            test['arimax_predictions'] = arimax_results[variant]['predictions']\n",
    "        \n",
    "        test.to_csv(f'{variant}_predictions.csv', index=False)\n",
    "        print(f\"{variant} predictions saved to '{variant}_predictions.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}