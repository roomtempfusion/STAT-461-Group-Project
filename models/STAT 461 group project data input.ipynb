{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad028a0-df7b-4acc-a109-5b73537a6f66",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e597ba-783f-4c81-816c-7192ff4d61bd",
   "metadata": {},
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import dropout_adj, to_dense_adj, dense_to_sparse, to_undirected, add_self_loops\n",
    "from torch_geometric.nn import SAGEConv, GraphConv\n",
    "import torch_geometric.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbe3d40-9f38-4954-b72f-3a6258abd344",
   "metadata": {},
   "source": [
    "airport = pd.read_csv('airport_adj_matrix.csv', index_col=0)\n",
    "border = pd.read_csv('border_adj_matrix.csv', index_col=0)\n",
    "cases_and_exogenous = pd.read_csv('cases_and_exogenous_il.csv')\n",
    "combined_prevalence_and_exogenous = pd.read_csv('combined_prevalence_and_exogenous.csv')\n",
    "highway_adj_matrix = pd.read_csv('highway_adj_matrix.csv', index_col=0)\n",
    "r0 = pd.read_csv('r0.csv', index_col=0)\n",
    "combined_covariates_interpolated = pd.read_csv('combined_covariates_interpolated.csv')\n",
    "pivoted_prevalences = pd.read_csv('pivoted_prevalences.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5474ca7-280a-474f-9b46-b5d012c8c46d",
   "metadata": {},
   "source": [
    "vals = border.to_numpy(dtype=float)\n",
    "mask = vals != 0\n",
    "\n",
    "if mask.any():\n",
    "    vmin = vals[mask].min()\n",
    "    vmax = vals[mask].max()\n",
    "    if vmax > vmin:\n",
    "        vals[mask] = (vals[mask] - vmin) / (vmax - vmin)\n",
    "    else:\n",
    "        # all nonzeros are equal → map them to 1\n",
    "        vals[mask] = 1.0\n",
    "\n",
    "border_scaled = pd.DataFrame(vals, index=border.index, columns=border.columns)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f167e1e7-1bd0-4ed1-b995-93b22d1b5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = highway_adj_matrix.to_numpy(dtype=float)\n",
    "mask = vals != 0\n",
    "\n",
    "if mask.any():\n",
    "    vmin = vals[mask].min()\n",
    "    vmax = vals[mask].max()\n",
    "    if vmax > vmin:\n",
    "        vals[mask] = (vals[mask] - vmin) / (vmax - vmin)\n",
    "    else:\n",
    "        # all nonzeros are equal → map them to 1\n",
    "        vals[mask] = 1.0\n",
    "\n",
    "highway_adj_matrix_scaled = pd.DataFrame(vals, index=highway_adj_matrix.index, columns=highway_adj_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0fb5e53-80f8-4103-be3a-ecf7c6e66f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#border_scaled.to_csv(\"border_scaled.csv\", index=True) \n",
    "#highway_adj_matrix_scaled.to_csv(\"highway_adj_matrix_scaled.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b551807-e230-455b-ac47-fc0723f59c4e",
   "metadata": {},
   "source": [
    "VARIANTS  = ['Alpha', 'Delta', 'Epsilon', 'Iota']\n",
    "df = combined_prevalence_and_exogenous.copy()\n",
    "df = df.sort_values(['location', 'date'])\n",
    "\n",
    "non_key = [c for c in df.columns if c not in ['location', 'date']]\n",
    "df[non_key] = df.groupby('location', group_keys=False)[non_key].ffill().bfill()\n",
    "\n",
    "# Rebase the index to 0..n-1\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "combined_prevalence_and_exogenous = df\n",
    "combined_prevalence_and_exogenous['Others'] = 1 - combined_prevalence_and_exogenous[VARIANTS].sum(axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f0cf422-e640-4c6c-ad0a-8c7f47c35d05",
   "metadata": {},
   "source": [
    "mobility = airport + highway_adj_matrix + border_scaled"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff8f99cc-6a4b-4726-9ba7-24bf557a4e25",
   "metadata": {},
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "FEATURE_COLS = ['cases_pc_7d','deaths_pc_7d']  # put your daily node features here (created below)\n",
    "VARIANTS      = ['Alpha', 'Beta', 'Delta', 'Epsilon', 'Gamma', 'Iota', 'Omicron']  # edit to your columns\n",
    "R0_DICT       = {'Alpha': 1.19, 'Beta': 1.15, 'Gamma': 1.21, 'Delta': 1.17, 'Epsilon':1.12, 'Iota':1.16, 'Omicron': 1.53}\n",
    "T_IN          = 28\n",
    "HORIZON       = 1\n",
    "EPOCHS        = 50\n",
    "LR            = 1e-3\n",
    "BATCH_SIZE    = 1\n",
    "TRAIN_RATIO   = 0.7\n",
    "VAL_RATIO     = 0.1\n",
    "PER_CAPITA    = None\n",
    "SMOOTH_DAYS   = 7      # rolling mean window for stability"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fed5e73-a2d8-45fa-923e-81655c6aa1a5",
   "metadata": {},
   "source": [
    "df = combined_covariates_interpolated.copy()\n",
    "df = df.sort_values(['location', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Columns to treat specially\n",
    "special = [c for c in ['confirmed', 'deaths', 'people_vaccinated', 'people_fully_vaccinated'] if c in df.columns]\n",
    "non_key = [c for c in df.columns if c not in ['location', 'date']]\n",
    "others = [c for c in non_key if c not in special]\n",
    "\n",
    "# 1) ffill/bfill for non-special columns\n",
    "if others:\n",
    "    df[others] = (\n",
    "        df.groupby('location', group_keys=False)[others]\n",
    "          .ffill().bfill()\n",
    "    )\n",
    "\n",
    "# 2) linear interpolation for cumulative columns (confirmed, deaths)\n",
    "if special:\n",
    "    # ensure numeric\n",
    "    df[special] = df[special].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    df[special] = (\n",
    "        df.groupby('location', group_keys=False)[special]\n",
    "          .apply(lambda g: g.interpolate(method='linear', limit_direction='both'))\n",
    "    )\n",
    "\n",
    "    # (Optional) enforce monotonic non-decreasing per location\n",
    "    df[special] = (\n",
    "     df.groupby('location', group_keys=False)[special]\n",
    "           .apply(lambda g: g.clip(lower=g.cummax()))\n",
    "     )\n",
    "\n",
    "combined_covariates_interpolated = df\n",
    "\n",
    "# Recompute 'Others' share if you have VARIANTS columns\n",
    "combined_covariates_interpolated['Others'] = (\n",
    "    1 - combined_covariates_interpolated[VARIANTS].sum(axis=1)\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b5d7d-7e3f-48a5-b472-057d9de3e6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c555be7-30c4-411a-ba74-ce82812dabee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa3faca1-1319-48d9-bdb5-43a4908b4168",
   "metadata": {},
   "source": [
    "def build_graph_from_adj(adj_df: pd.DataFrame, county_names: list = None, undirected=True, add_loops=True):\n",
    "    A = torch.tensor(adj_df.values, dtype=torch.float32)\n",
    "    \n",
    "    # remove self entries; we will add later\n",
    "    A.fill_diagonal_(0.)\n",
    "    edge_index, edge_weight = dense_to_sparse(A)  # mobility as weights\n",
    "    if undirected:\n",
    "        edge_index, edge_weight = to_undirected(edge_index, edge_weight, reduce='mean')\n",
    "    if add_loops:\n",
    "        edge_index, edge_weight = add_self_loops(edge_index, edge_weight, num_nodes=A.size(0), fill_value=1.0)\n",
    "    data = Data(edge_index=edge_index, edge_weight=edge_weight, num_nodes=A.size(0))\n",
    "    data.county_names = county_names\n",
    "    return data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4507ec17-0a00-4d3c-ba54-f5184a67fd38",
   "metadata": {},
   "source": [
    "def build_XY(\n",
    "    df: pd.DataFrame,\n",
    "    county_names: list,\n",
    "    variant_cols: list,           # e.g. ['alpha','beta','gamma','delta','omicron']\n",
    "    r0_dict: dict = None,         # kept for API compatibility; ignored\n",
    "    feature_cols_out: list = None,# e.g. ['cases_pc_7d','deaths_pc_7d','mobility_in', ...]\n",
    "    county_col='location',\n",
    "    date_col='date',\n",
    "    confirmed_col='confirmed',\n",
    "    death_col='deaths',\n",
    "    pop_col='population',\n",
    "    per_capita=1e5,               # None to keep raw counts\n",
    "    smooth_days=7,                # rolling mean window\n",
    "    fill_method='ffill_then_zero',# 'ffill_then_zero' | 'ffill' | 'zero' | None\n",
    "    renormalize_variant_shares=True,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X_all: [T, N, F]   using exactly `feature_cols_out` (no r0_wavg)\n",
    "      Y_all: [T, N, K]   per-variant incidence (per-capita, smoothed)\n",
    "      dates: DatetimeIndex of length T\n",
    "    \"\"\"\n",
    "    if feature_cols_out is None:\n",
    "        feature_cols_out = []\n",
    "\n",
    "    d = df.copy()\n",
    "\n",
    "    # --- normalize keys ---\n",
    "    d[county_col] = d[county_col].astype(str).str.strip()\n",
    "    d[date_col]   = pd.to_datetime(d[date_col]).dt.normalize()\n",
    "    d = d[d[county_col].isin(set(county_names))]\n",
    "    d = d.sort_values([county_col, date_col])\n",
    "    if d.empty:\n",
    "        raise ValueError(\"No rows after filtering to county_names. Check names/casing.\")\n",
    "\n",
    "    # --- ensure variant columns exist; coerce numeric and clip to [0,1] ---\n",
    "    for v in variant_cols:\n",
    "        if v not in d.columns:\n",
    "            if verbose: print(f\"[WARN] missing variant column '{v}', filling with 0.\")\n",
    "            d[v] = 0.0\n",
    "    if variant_cols:\n",
    "        d[variant_cols] = d[variant_cols].apply(pd.to_numeric, errors='coerce').clip(0.0, 1.0)\n",
    "        if renormalize_variant_shares:\n",
    "            share_sum = d[variant_cols].sum(axis=1)\n",
    "            mask = share_sum > 0\n",
    "            d.loc[mask, variant_cols] = d.loc[mask, variant_cols].div(share_sum[mask], axis=0)\n",
    "            # rows with all-zeros stay zeros (unknown/none)\n",
    "\n",
    "    # --- daily new cases/deaths from cumulative ---\n",
    "    d['new_cases']  = d.groupby(county_col)[confirmed_col].diff().clip(lower=0).fillna(0)\n",
    "    if death_col in d.columns:\n",
    "        d['new_deaths'] = d.groupby(county_col)[death_col].diff().clip(lower=0).fillna(0)\n",
    "    else:\n",
    "        d['new_deaths'] = 0.0\n",
    "\n",
    "    # --- per-capita scaling ---\n",
    "    if (pop_col in d.columns) and (per_capita is not None):\n",
    "        denom = d[pop_col].replace(0, np.nan)\n",
    "        d['cases_pc']  = per_capita * d['new_cases']  / denom\n",
    "        d['deaths_pc'] = per_capita * d['new_deaths'] / denom\n",
    "    else:\n",
    "        d['cases_pc']  = d['new_cases']\n",
    "        d['deaths_pc'] = d['new_deaths']\n",
    "\n",
    "    # --- smoothing ---\n",
    "    if smooth_days and smooth_days > 1:\n",
    "        for col in ['cases_pc', 'deaths_pc']:\n",
    "            d[f'{col}_7d'] = (\n",
    "                d.groupby(county_col)[col]\n",
    "                 .rolling(smooth_days, min_periods=1).mean()\n",
    "                 .reset_index(level=0, drop=True)\n",
    "            )\n",
    "    else:\n",
    "        d['cases_pc_7d']  = d['cases_pc']\n",
    "        d['deaths_pc_7d'] = d['deaths_pc']\n",
    "\n",
    "    # --- fill strategy BEFORE pivoting (reduce NaNs) ---\n",
    "    feature_cols = list(feature_cols_out)  # EXACT features for X\n",
    "    cols_to_fill = list(set(feature_cols + ['cases_pc_7d']))\n",
    "    cols_to_fill = [c for c in cols_to_fill if c in d.columns]\n",
    "    if fill_method in ('ffill', 'ffill_then_zero'):\n",
    "        d[cols_to_fill] = d.groupby(county_col)[cols_to_fill].ffill()\n",
    "        if fill_method == 'ffill_then_zero':\n",
    "            d[cols_to_fill] = d[cols_to_fill].fillna(0.0)\n",
    "    elif fill_method == 'zero':\n",
    "        d[cols_to_fill] = d[cols_to_fill].fillna(0.0)\n",
    "\n",
    "    # --- timeline & pivot helper ---\n",
    "    dates = np.sort(d[date_col].unique())\n",
    "    if len(dates) == 0:\n",
    "        raise ValueError(\"No dates after normalization.\")\n",
    "\n",
    "    def pivot_TN(col):\n",
    "        if col not in d.columns:\n",
    "            if verbose: print(f\"[WARN] missing {col}, filling zeros.\")\n",
    "            return np.zeros((len(dates), len(county_names)), dtype='float32')\n",
    "        wide = (\n",
    "            d.pivot_table(index=county_col, columns=date_col, values=col, aggfunc='mean')\n",
    "             .reindex(index=county_names, columns=dates)\n",
    "        ).fillna(0.0)\n",
    "        return wide.values.T.astype('float32')   # [T, N]\n",
    "\n",
    "    # --- X: stack ONLY requested features ---\n",
    "    X_all = torch.tensor(\n",
    "        np.stack([pivot_TN(c) for c in feature_cols], axis=-1) if feature_cols else\n",
    "        np.zeros((len(dates), len(county_names), 0), dtype='float32'),\n",
    "        dtype=torch.float32\n",
    "    )  # [T,N,F]\n",
    "\n",
    "    # --- Y: per-variant incidence (per-capita, smoothed) = cases_pc_7d * share_v ---\n",
    "    for v in variant_cols:\n",
    "        d[f'{v}_incidence_pc_7d'] = d['cases_pc_7d'].fillna(0.0) * d[v].fillna(0.0)\n",
    "\n",
    "    Y_stack = np.stack([pivot_TN(f'{v}_incidence_pc_7d') for v in variant_cols], axis=-1)  # [T,N,K]\n",
    "    Y_all = torch.tensor(Y_stack, dtype=torch.float32)\n",
    "\n",
    "    if verbose:\n",
    "        T, N, F = X_all.shape\n",
    "        _, _, K = Y_all.shape\n",
    "        print(f\"[OK] X_all: (T={T}, N={N}, F={F}); Y_all: (T={T}, N={N}, K={K})\")\n",
    "        nnzX = (X_all != 0).float().mean().item() if F > 0 else 0.0\n",
    "        nnzY = (Y_all != 0).float().mean().item()\n",
    "        print(f\"      nnz frac X={nnzX:.3f}, Y={nnzY:.3f}\")\n",
    "\n",
    "    return X_all, Y_all, pd.to_datetime(dates)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41f44a55-a56c-4b16-8a48-56d73ec65f19",
   "metadata": {},
   "source": [
    "county_names = mobility.columns.tolist()\n",
    "data = build_graph_from_adj(mobility, county_names, undirected=True, add_loops=True)\n",
    "\n",
    "X_all, Y_all, dates = build_XY(\n",
    "     combined_covariates_interpolated, county_names,\n",
    "     variant_cols=VARIANTS,r0_dict = R0_DICT ,\n",
    "     feature_cols_out=FEATURE_COLS,\n",
    "     county_col='location', date_col='date',\n",
    "     confirmed_col='confirmed', death_col='deaths', pop_col='population',\n",
    "     per_capita=None, smooth_days=None\n",
    " )\n",
    "\n",
    "r0_vec = [R0_DICT[v] for v in VARIANTS]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93f7ff0f-d9c4-4d5b-9441-1d2158ebdfb3",
   "metadata": {},
   "source": [
    "def make_windows(X_all, Y_all, L=28, horizon=1):\n",
    "    # X_all: [T,N,F], Y_all: [T,N,K]\n",
    "    Xs, Ys = [], []\n",
    "    for t in range(L, len(X_all) - horizon + 1):\n",
    "        Xs.append(X_all[t-L:t])                # [L,N,F]\n",
    "        Ys.append(Y_all[t + horizon - 1])      # [N,K] (predict next step)\n",
    "    # -> [B,L,N,F], [B,N,K]\n",
    "    return np.stack(Xs), np.stack(Ys)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fb36ae3-37ec-4d32-992d-53909fe57d3e",
   "metadata": {},
   "source": [
    "L = 28\n",
    "horizon = 1\n",
    "\n",
    "Xs, Ys = make_windows(X_all, Y_all, L=L, horizon=horizon)\n",
    "\n",
    "# Determine how many windows for training vs testing\n",
    "train_frac = 0.8  # 80% train, 20% test\n",
    "n_total = len(Xs)\n",
    "n_train = int(n_total * train_frac)\n",
    "\n",
    "# Split\n",
    "X_train, Y_train = Xs[:n_train], Ys[:n_train]\n",
    "X_test,  Y_test  = Xs[n_train:], Ys[n_train:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d7867e8a-e479-4b90-90c7-31818dd71761",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "county0 = county_names[0]  # name of the first county, if you have this list\n",
    "y = X_all[:, 0, 0].detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(9,3))\n",
    "plt.plot(dates, y)\n",
    "plt.title(f\"Feature 0 over time — {county0}\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Feature 0\"); plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d6b1e47-b259-4128-b5ad-75a811d532f9",
   "metadata": {},
   "source": [
    "county0 = county_names[1]  # name of the first county, if you have this list\n",
    "y = X_all[:, 1, 0].detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(9,3))\n",
    "plt.plot(dates, y)\n",
    "plt.title(f\"Feature 0 over time — {county0}\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Feature 0\"); plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28b02b4e-65f4-402f-a03f-a724f87eeba1",
   "metadata": {},
   "source": [
    "plt.plot(dates, Y_all[:,0,0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef0279c1-ba32-4363-a059-372c26cb935d",
   "metadata": {},
   "source": [
    "cut = pd.Timestamp('2023-03-24')\n",
    "\n",
    "# make sure dates is a pandas DatetimeIndex\n",
    "dates = pd.to_datetime(dates)\n",
    "\n",
    "# time mask\n",
    "tmask = dates >= cut                       # [T]\n",
    "\n",
    "# slice feature 0 over the time window: [T_sel, N]\n",
    "X_sel = X_all[tmask, :, 0]\n",
    "\n",
    "# counties with ANY nonzero entries after 2023-07\n",
    "nonzero_mask = (X_sel != 0).any(dim=0)     # [N] boolean\n",
    "\n",
    "# indices u\n",
    "u_idx = nonzero_mask.nonzero(as_tuple=True)[0].cpu().numpy().tolist()\n",
    "\n",
    "# names for readability\n",
    "u_names = [county_names[i] for i in u_idx]\n",
    "\n",
    "print(\"Indices:\", u_idx)\n",
    "print(\"Counties:\", u_names)\n",
    "\n",
    "# (optional) how many nonzero time-steps per county in that window\n",
    "nonzero_counts = (X_sel != 0).sum(dim=0).cpu().numpy()\n",
    "for i in u_idx[:10]:  # show first 10\n",
    "    print(county_names[i], int(nonzero_counts[i]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e244a56f-2646-4526-bb4d-c1a654d9ebcf",
   "metadata": {},
   "source": [
    "cut = pd.Timestamp('2023-03-24')\n",
    "\n",
    "# make sure dates is a pandas DatetimeIndex\n",
    "dates = pd.to_datetime(dates)\n",
    "\n",
    "# time mask\n",
    "tmask = dates >= cut                       # [T]\n",
    "\n",
    "# slice feature 0 over the time window: [T_sel, N]\n",
    "Y_sel = Y_all[tmask, :, 0]\n",
    "\n",
    "# counties with ANY nonzero entries after 2023-07\n",
    "nonzero_mask = (Y_sel != 0).any(dim=0)     # [N] boolean\n",
    "\n",
    "# indices u\n",
    "u_idx = nonzero_mask.nonzero(as_tuple=True)[0].cpu().numpy().tolist()\n",
    "\n",
    "# names for readability\n",
    "u_names = [county_names[i] for i in u_idx]\n",
    "\n",
    "print(\"Indices:\", u_idx)\n",
    "print(\"Counties:\", u_names)\n",
    "\n",
    "# (optional) how many nonzero time-steps per county in that window\n",
    "nonzero_counts = (Y_sel != 0).sum(dim=0).cpu().numpy()\n",
    "for i in u_idx[:10]:  # show first 10\n",
    "    print(county_names[i], int(nonzero_counts[i]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7736b68c-41c8-4700-91cd-80aa5b53a624",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "class STGCN_Hurdle_K(nn.Module):\n",
    "    def __init__(self, in_f, hid_f=64, K=3, cheb_K=3, n_blocks=2):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            STGCNBlock(in_f if i==0 else hid_f, hid_f, K=cheb_K, t_kernel=3, t_dilation=2**i)\n",
    "            for i in range(n_blocks)\n",
    "        ])\n",
    "        # Heads: map hidden -> K parameters per node\n",
    "        self.head_p  = nn.Linear(hid_f, K)   # sigmoid → [B,N,K]\n",
    "        self.head_mu = nn.Linear(hid_f, K)   # softplus → [B,N,K]\n",
    "        self.head_r  = nn.Linear(hid_f, K)   # softplus → [B,N,K]\n",
    "\n",
    "    def forward(self, X, edge_index, edge_weight=None):\n",
    "        # X: [B,T,N,F]\n",
    "        H = X\n",
    "        for b in self.blocks:\n",
    "            H = b(H, edge_index, edge_weight) + H\n",
    "        H_last = H[:, -1]                    # [B,N,hid_f]\n",
    "        p  = torch.sigmoid(self.head_p(H_last))\n",
    "        mu = F.softplus(self.head_mu(H_last)) + 1e-6\n",
    "        r  = F.softplus(self.head_r(H_last)) + 1e-6\n",
    "        return p, mu, r\n",
    "\"\"\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7e07ea4-8d7d-4c81-bb57-4c683a80c07d",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "def nb_nll(y, mu, r, eps=1e-8):\n",
    "    # y, mu, r broadcastable to [B,N,K]\n",
    "    return (torch.lgamma(y + r) - torch.lgamma(r) - torch.lgamma(y+1)\n",
    "            + r*torch.log(r+eps) + y*torch.log(mu+eps)\n",
    "            - (y+r)*torch.log(mu+r+eps))\n",
    "\n",
    "def hurdle_loss(Y, p, mu, r, valid_mask=None):\n",
    "    # Y: [B,N,K] counts (>=0). valid_mask: [B,N,K] booleans for missing labels.\n",
    "    if valid_mask is None:\n",
    "        valid_mask = torch.ones_like(Y, dtype=torch.bool)\n",
    "    pos = (Y > 0).float()\n",
    "    # BCE on nonzero indicator\n",
    "    bce = F.binary_cross_entropy(p[valid_mask], pos[valid_mask], reduction='mean')\n",
    "    # NB only on positives\n",
    "    pmask = valid_mask & (Y > 0)\n",
    "    nb = nb_nll(Y[pmask], mu[pmask], r[pmask]).mean() if pmask.any() else 0.0\n",
    "    return bce + nb\n",
    "\"\"\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881dd392-90b9-4bd1-a146-4358214cfdb2",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a40ec0-66b5-4400-a9f1-a2882097d513",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
