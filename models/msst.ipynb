{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### README",
   "id": "81221e98587b9220"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model currently only has GRU active, the msst_classes file has the micro graphs turned off in the fusion module, and the only used features are the variants, not the exogenous covariates.",
   "id": "3fee037bd6c95243"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.366121Z",
     "start_time": "2025-11-13T10:48:56.028443Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import numpy as np\n",
    "from math import prod\n",
    "from torch.utils.data import Dataset\n",
    "import msst_classes \n",
    "from torch.utils.data import DataLoader, Subset"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Documents\\courses\\.venv\\lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "C:\\Documents\\courses\\.venv\\lib\\site-packages\\torch_geometric\\typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "C:\\Documents\\courses\\.venv\\lib\\site-packages\\torch_geometric\\typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(\n",
      "C:\\Documents\\courses\\.venv\\lib\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load State Data",
   "id": "65e6fa721fafaa64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.529366Z",
     "start_time": "2025-11-13T10:48:59.367121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_df = pd.read_csv(\"../processed data/state_level/rolled_covariates_state_level.csv\", parse_dates=['date'], dtype={'location': str})\n",
    "state_df.location = state_df.location.str.zfill(2)\n",
    "state_df.head()"
   ],
   "id": "640e30d6f0cac0e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        date location  people_vaccinated  people_fully_vaccinated  \\\n",
       "0 2021-01-01       01           53829.00                   272.00   \n",
       "1 2021-01-02       01           53940.50                   276.50   \n",
       "2 2021-01-03       01           54309.00                   279.00   \n",
       "3 2021-01-04       01           56004.75                   309.75   \n",
       "4 2021-01-05       01           58707.80                   416.60   \n",
       "\n",
       "   school_closing  workplace_closing  cancel_events  gatherings_restrictions  \\\n",
       "0             2.0                1.0            1.0                      0.0   \n",
       "1             2.0                1.0            1.0                      0.0   \n",
       "2             2.0                1.0            1.0                      0.0   \n",
       "3             2.0                1.0            1.0                      0.0   \n",
       "4             2.0                1.0            1.0                      0.0   \n",
       "\n",
       "   transport_closing  stay_home_restrictions  ...  population  Alpha  Beta  \\\n",
       "0                0.0                     1.0  ...   4903185.0    0.0   0.0   \n",
       "1                0.0                     1.0  ...   4903185.0    0.0   0.0   \n",
       "2                0.0                     1.0  ...   4903185.0    0.0   0.0   \n",
       "3                0.0                     1.0  ...   4903185.0    0.0   0.0   \n",
       "4                0.0                     1.0  ...   4903185.0    0.0   0.0   \n",
       "\n",
       "   Delta  Epsilon  Gamma  Iota  Omicron     deaths        Other  \n",
       "0    0.0      0.0    0.0   0.0      0.0  45.000000  4521.000000  \n",
       "1    0.0      0.0    0.0   0.0      0.0  22.500000  4116.000000  \n",
       "2    0.0      0.0    0.0   0.0      0.0  15.333333  3569.333333  \n",
       "3    0.0      0.0    0.0   0.0      0.0  12.750000  3217.250000  \n",
       "4    0.0      0.0    0.0   0.0      0.0  11.800000  3673.400000  \n",
       "\n",
       "[5 rows x 32 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>people_vaccinated</th>\n",
       "      <th>people_fully_vaccinated</th>\n",
       "      <th>school_closing</th>\n",
       "      <th>workplace_closing</th>\n",
       "      <th>cancel_events</th>\n",
       "      <th>gatherings_restrictions</th>\n",
       "      <th>transport_closing</th>\n",
       "      <th>stay_home_restrictions</th>\n",
       "      <th>...</th>\n",
       "      <th>population</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>Iota</th>\n",
       "      <th>Omicron</th>\n",
       "      <th>deaths</th>\n",
       "      <th>Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>01</td>\n",
       "      <td>53829.00</td>\n",
       "      <td>272.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4903185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>01</td>\n",
       "      <td>53940.50</td>\n",
       "      <td>276.50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4903185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>4116.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-03</td>\n",
       "      <td>01</td>\n",
       "      <td>54309.00</td>\n",
       "      <td>279.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4903185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>3569.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>01</td>\n",
       "      <td>56004.75</td>\n",
       "      <td>309.75</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4903185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>3217.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>01</td>\n",
       "      <td>58707.80</td>\n",
       "      <td>416.60</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4903185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>3673.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.551505Z",
     "start_time": "2025-11-13T10:48:59.529366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_covs = pd.read_csv(\"../processed data/state_level/state_level_characteristics.csv\", dtype={'state_fips':str}, index_col=0)\n",
    "state_covs.head()"
   ],
   "id": "b7ea126bb4902671",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  state_fips  Population  median_age  Income  Density_per_mile\n",
       "0         01     5157699        39.6   65560             101.0\n",
       "1         04     7582384        39.4   84700              65.0\n",
       "2         05     3088354        39.1   64840              59.0\n",
       "3         06    39431263        38.4  100600             250.0\n",
       "4         08     5957494        38.0  106500              57.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_fips</th>\n",
       "      <th>Population</th>\n",
       "      <th>median_age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Density_per_mile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>5157699</td>\n",
       "      <td>39.6</td>\n",
       "      <td>65560</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04</td>\n",
       "      <td>7582384</td>\n",
       "      <td>39.4</td>\n",
       "      <td>84700</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05</td>\n",
       "      <td>3088354</td>\n",
       "      <td>39.1</td>\n",
       "      <td>64840</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06</td>\n",
       "      <td>39431263</td>\n",
       "      <td>38.4</td>\n",
       "      <td>100600</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08</td>\n",
       "      <td>5957494</td>\n",
       "      <td>38.0</td>\n",
       "      <td>106500</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.582751Z",
     "start_time": "2025-11-13T10:48:59.551505Z"
    }
   },
   "cell_type": "code",
   "source": "state_df = state_df.merge(state_covs, left_on='location', right_on='state_fips', how='inner').drop(['population', 'deaths', 'state_fips'], axis=1)",
   "id": "dd995a6e3cdef7f4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.598374Z",
     "start_time": "2025-11-13T10:48:59.582751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_fips_list = sorted(state_df['location'].unique())\n",
    "state_index = {fips: i for i, fips in enumerate(state_fips_list)}\n",
    "S = len(state_fips_list)"
   ],
   "id": "9dc47c224e81bb4a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.613999Z",
     "start_time": "2025-11-13T10:48:59.598374Z"
    }
   },
   "cell_type": "code",
   "source": "all_dates = pd.date_range(state_df['date'].min(), state_df['date'].max(), freq='D')",
   "id": "30b2d8de22546514",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.629622Z",
     "start_time": "2025-11-13T10:48:59.613999Z"
    }
   },
   "cell_type": "code",
   "source": "state_df = state_df.sort_values(['date', 'location'])",
   "id": "fb43a9be4f7c0736",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.651759Z",
     "start_time": "2025-11-13T10:48:59.629622Z"
    }
   },
   "cell_type": "code",
   "source": "state_df = state_df.set_index(['location', 'date'])",
   "id": "3c21314ed6966c8e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.667383Z",
     "start_time": "2025-11-13T10:48:59.651759Z"
    }
   },
   "cell_type": "code",
   "source": "variant_cols = ['Alpha', 'Beta', 'Delta', 'Epsilon', 'Gamma', 'Iota', 'Omicron', 'Other']",
   "id": "8a05c498fead8f49",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.683006Z",
     "start_time": "2025-11-13T10:48:59.667383Z"
    }
   },
   "cell_type": "code",
   "source": "state_df[variant_cols] /= 1000",
   "id": "ee6525f18f5bd0f7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.698630Z",
     "start_time": "2025-11-13T10:48:59.683006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "exo_cols = ['people_vaccinated', 'people_fully_vaccinated', 'school_closing',\n",
    "       'workplace_closing', 'cancel_events', 'gatherings_restrictions',\n",
    "       'transport_closing', 'stay_home_restrictions',\n",
    "       'internal_movement_restrictions', 'international_movement_restrictions',\n",
    "       'information_campaigns', 'testing_policy', 'contact_tracing',\n",
    "       'facial_coverings', 'vaccination_policy', 'elderly_people_protection',\n",
    "       'government_response_index', 'stringency_index',\n",
    "       'containment_health_index', 'economic_support_index']\n",
    "static_cols = ['Population',\n",
    "       'median_age', 'Income', 'Density_per_mile']"
   ],
   "id": "199252ef9fb6bf73",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Key testing change - we've set feature cols to only have variants",
   "id": "20f3c09fe604ce81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.714253Z",
     "start_time": "2025-11-13T10:48:59.698630Z"
    }
   },
   "cell_type": "code",
   "source": "feature_cols = variant_cols",
   "id": "2812d7e5e44883d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.729877Z",
     "start_time": "2025-11-13T10:48:59.714253Z"
    }
   },
   "cell_type": "code",
   "source": "# feature_cols = variant_cols + static_cols + exo_cols",
   "id": "db30eeff5b39a6c1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.809118Z",
     "start_time": "2025-11-13T10:48:59.729877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "T = len(all_dates)\n",
    "F_macro = len(feature_cols)\n",
    "X_state = np.zeros((T, S, F_macro), dtype=np.float32)\n",
    "\n",
    "for s_idx, s_fips in enumerate(state_fips_list):\n",
    "    sub = state_df.xs(s_fips, level='location')[feature_cols]\n",
    "    X_state[:, s_idx, :] = sub.to_numpy()\n"
   ],
   "id": "f60ba3fb6c781587",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build County Adjacency Matrix",
   "id": "57d979927a33c60d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.824740Z",
     "start_time": "2025-11-13T10:48:59.809118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../processed data/county level/county_adj_by_state.pkl', 'rb') as file:\n",
    "    adj_1 = pickle.load(file)"
   ],
   "id": "9abcb5adc077b1ac",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.851877Z",
     "start_time": "2025-11-13T10:48:59.824740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../processed data/county level/county_airport_weights_by_state.pkl', 'rb') as file:\n",
    "    adj_2 = pickle.load(file)"
   ],
   "id": "26a310a6e15ffaad",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.867502Z",
     "start_time": "2025-11-13T10:48:59.851877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../processed data/county level/county_highway_weights_by_state.pkl', 'rb') as file:\n",
    "    adj_3 = pickle.load(file)"
   ],
   "id": "1ee26c3cd2791be2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.883124Z",
     "start_time": "2025-11-13T10:48:59.867502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "county_ids_dict = {}   # state_fips -> ordered list of county FIPS (strings or ints)\n",
    "county_adj_dict = {}   # state_fips -> np.ndarray (M_s x M_s)"
   ],
   "id": "9c58f8422c9879ae",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.898460Z",
     "start_time": "2025-11-13T10:48:59.885387Z"
    }
   },
   "cell_type": "code",
   "source": "state_fips_list = sorted(adj_1.keys())",
   "id": "aa0bffdda6a03abb",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:48:59.918117Z",
     "start_time": "2025-11-13T10:48:59.900461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_df(df):\n",
    "    arr = df.to_numpy(dtype=np.float32)\n",
    "    max_val = np.percentile(arr, 95)  # or arr.max()\n",
    "    if max_val > 0:\n",
    "        arr = arr / max_val\n",
    "    return arr"
   ],
   "id": "cc662e2cc0884795",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:00.011807Z",
     "start_time": "2025-11-13T10:48:59.918117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for s_fips in state_fips_list:\n",
    "    df1 = adj_1[s_fips]\n",
    "    df2 = adj_2.get(s_fips, None)          # may be None\n",
    "    df3 = adj_3.get(s_fips, None)          # may be None\n",
    "\n",
    "    # 1) Compute the union of county IDs present in any of the three\n",
    "    counties = set(df1.index)\n",
    "    if df2 is not None:\n",
    "        counties |= set(df2.index)\n",
    "    if df3 is not None:\n",
    "        counties |= set(df3.index)\n",
    "    counties = sorted(counties)\n",
    "\n",
    "    # 2) Align each DF to this county list; missing df gets all zeros\n",
    "    df1_al = df1.reindex(index=counties, columns=counties, fill_value=0)\n",
    "\n",
    "    if df2 is not None:\n",
    "        df2_al = df2.reindex(index=counties, columns=counties, fill_value=0)\n",
    "    else:\n",
    "        df2_al = pd.DataFrame(0.0, index=counties, columns=counties)\n",
    "\n",
    "    if df3 is not None:\n",
    "        df3_al = df3.reindex(index=counties, columns=counties, fill_value=0)\n",
    "    else:\n",
    "        df3_al = pd.DataFrame(0.0, index=counties, columns=counties)\n",
    "\n",
    "    # 3) Sum the three weight types\n",
    "    W1 = normalize_df(df1_al)\n",
    "    W2 = normalize_df(df2_al) if df2 is not None else np.zeros_like(W1)\n",
    "    W3 = normalize_df(df3_al) if df3 is not None else np.zeros_like(W1)\n",
    "    \n",
    "    # Option A: equal weight\n",
    "    A_s = W1 + W2 + W3\n",
    "\n",
    "    # 4) Save ordering + adjacency\n",
    "    county_ids_dict[s_fips] = counties   # this defines row/col order\n",
    "    county_adj_dict[s_fips] = A_s        # (M_s, M_s)"
   ],
   "id": "e6134bf3649021e9",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build global county graph",
   "id": "4375d29400c8585b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:00.052065Z",
     "start_time": "2025-11-13T10:49:00.011807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "county_fips_global = []   # global list of county FIPS\n",
    "state_of_county = []      # same length as county_fips_global\n",
    "\n",
    "# Also count total counties to size A_county_global later\n",
    "total_counties = 0\n",
    "for s_fips in state_fips_list:\n",
    "    counties_s = county_ids_dict[s_fips]\n",
    "    total_counties += len(counties_s)\n",
    "\n",
    "M = total_counties\n",
    "state_of_county = np.zeros(M, dtype=np.int64)\n",
    "\n",
    "# Fill county_fips_global + state_of_county and build A_county_global\n",
    "A_county_global = np.zeros((M, M), dtype=np.float32)\n",
    "\n",
    "offset = 0\n",
    "for s_fips in state_fips_list:\n",
    "    A_s = county_adj_dict[s_fips].astype(np.float32)\n",
    "    counties_s = county_ids_dict[s_fips]\n",
    "    n_s = len(counties_s)\n",
    "\n",
    "    # sanity check\n",
    "    assert A_s.shape == (n_s, n_s), f\"Adjacency size mismatch for state {s_fips}\"\n",
    "\n",
    "    # record county FIPS + state indices for these n_s counties\n",
    "    s_idx = state_index[s_fips]\n",
    "    for local_idx, c_fips in enumerate(counties_s):\n",
    "        global_idx = offset + local_idx\n",
    "        county_fips_global.append(c_fips)\n",
    "        state_of_county[global_idx] = s_idx\n",
    "\n",
    "    # place this state's adjacency in the block-diagonal\n",
    "    A_county_global[offset:offset+n_s, offset:offset+n_s] = A_s\n",
    "\n",
    "    offset += n_s\n",
    "\n",
    "# final sanity\n",
    "assert offset == M\n"
   ],
   "id": "43a52759fad6d9a8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Building covariates",
   "id": "cc0b3d48a1aa5575"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:00.067689Z",
     "start_time": "2025-11-13T10:49:00.052065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "county_fips_global = [str(c) for c in county_fips_global]\n",
    "county_to_global = {c_fips: idx for idx, c_fips in enumerate(county_fips_global)}\n",
    "\n",
    "M = len(county_fips_global)\n",
    "T = len(all_dates)\n",
    "F_micro = len(variant_cols)\n"
   ],
   "id": "183bd7587435bb01",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:00.560451Z",
     "start_time": "2025-11-13T10:49:00.067689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../processed data/county level/rolled_county_cases.pkl', 'rb') as file:\n",
    "    county_cases_dict = pickle.load(file)"
   ],
   "id": "bc8fc73f32a8b58f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:00.631534Z",
     "start_time": "2025-11-13T10:49:00.560451Z"
    }
   },
   "cell_type": "code",
   "source": "county_cases_dict['01']",
   "id": "cba2fd549112968",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        fips       date state  Alpha  Beta  Delta  Epsilon  Gamma  Iota  \\\n",
       "0      01001 2021-01-01    01    0.0   0.0    0.0      0.0    0.0   0.0   \n",
       "1      01001 2021-01-02    01    0.0   0.0    0.0      0.0    0.0   0.0   \n",
       "2      01001 2021-01-03    01    0.0   0.0    0.0      0.0    0.0   0.0   \n",
       "3      01001 2021-01-04    01    0.0   0.0    0.0      0.0    0.0   0.0   \n",
       "4      01001 2021-01-05    01    0.0   0.0    0.0      0.0    0.0   0.0   \n",
       "...      ...        ...   ...    ...   ...    ...      ...    ...   ...   \n",
       "49635  01999 2022-12-27    01    0.0   0.0    0.0      0.0    0.0   0.0   \n",
       "49636  01999 2022-12-28    01    0.0   0.0    0.0      0.0    0.0   0.0   \n",
       "49637  01999 2022-12-29    01    0.0   0.0    0.0      0.0    0.0   0.0   \n",
       "49638  01999 2022-12-30    01    0.0   0.0    0.0      0.0    0.0   0.0   \n",
       "49639  01999 2022-12-31    01    0.0   0.0    0.0      0.0    0.0   0.0   \n",
       "\n",
       "          Omicron       Other  \n",
       "0        0.000000   49.000000  \n",
       "1        0.000000   39.000000  \n",
       "2        0.000000   38.333333  \n",
       "3        0.000000   36.500000  \n",
       "4        0.000000   71.200000  \n",
       "...           ...         ...  \n",
       "49635    0.000000    0.000000  \n",
       "49636    0.000000    0.000000  \n",
       "49637  537.485714  134.371429  \n",
       "49638  537.485714  134.371429  \n",
       "49639  537.485714  134.371429  \n",
       "\n",
       "[49640 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>Iota</th>\n",
       "      <th>Omicron</th>\n",
       "      <th>Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01001</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01001</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01001</td>\n",
       "      <td>2021-01-03</td>\n",
       "      <td>01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01001</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01001</td>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49635</th>\n",
       "      <td>01999</td>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49636</th>\n",
       "      <td>01999</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49637</th>\n",
       "      <td>01999</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>537.485714</td>\n",
       "      <td>134.371429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49638</th>\n",
       "      <td>01999</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>537.485714</td>\n",
       "      <td>134.371429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49639</th>\n",
       "      <td>01999</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>537.485714</td>\n",
       "      <td>134.371429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49640 rows × 11 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:06.251014Z",
     "start_time": "2025-11-13T10:49:00.633456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_county = np.zeros((T, M, F_micro), dtype=np.float32)\n",
    "\n",
    "for s_fips in state_fips_list:\n",
    "    if s_fips not in county_cases_dict:\n",
    "        # If a state has adjacency but no county time series -> stays all zeros\n",
    "        continue\n",
    "\n",
    "    df = county_cases_dict[s_fips].copy()\n",
    "    # ensure types & ordering\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['fips'] = df['fips'].astype(str)\n",
    "    df[variant_cols] /= 1000\n",
    "    # (optional) sanity: all counties from this state should be in county_ids_dict\n",
    "    # missing ones will just be ignored or raise a KeyError below\n",
    "    # assert set(df['county_fips'].unique()).issubset(set(county_ids_dict[s_fips]))\n",
    "\n",
    "    # group by county and fill each global row\n",
    "    for c_fips, group in df.groupby('fips'):\n",
    "        # skip counties that didn't make it into the adjacency for some reason\n",
    "        if c_fips not in county_to_global:\n",
    "            continue\n",
    "\n",
    "        g_idx = county_to_global[c_fips]\n",
    "\n",
    "        # index by date, select variant columns, align to all_dates\n",
    "        sub = (\n",
    "            group\n",
    "            .set_index('date')[variant_cols]\n",
    "            .reindex(all_dates)\n",
    "            .fillna(0.0)\n",
    "        )\n",
    "\n",
    "        X_county[:, g_idx, :] = sub.to_numpy(dtype=np.float32)\n"
   ],
   "id": "de8f430010402e86",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:06.266641Z",
     "start_time": "2025-11-13T10:49:06.251014Z"
    }
   },
   "cell_type": "code",
   "source": "X_state.shape",
   "id": "169af95fe681e8e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 49, 8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:06.293472Z",
     "start_time": "2025-11-13T10:49:06.269787Z"
    }
   },
   "cell_type": "code",
   "source": "X_county.shape",
   "id": "8fa8e40489817633",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 3109, 8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build A_state",
   "id": "b9905592b26f320c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:06.309096Z",
     "start_time": "2025-11-13T10:49:06.293472Z"
    }
   },
   "cell_type": "code",
   "source": "state_airweights = pd.read_csv('../processed data/state_level/state_level_airport_weights.csv', index_col=0, dtype={'STATEFP':str})",
   "id": "4d04fb4248f5c303",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:06.324719Z",
     "start_time": "2025-11-13T10:49:06.309096Z"
    }
   },
   "cell_type": "code",
   "source": "state_borderweights = pd.read_csv('../processed data/state_level/state_level_border_weights.csv', index_col=0, dtype={'state_fips':str})",
   "id": "e53b6064648cf247",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:06.340341Z",
     "start_time": "2025-11-13T10:49:06.324719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_highwayweights = pd.read_csv('../processed data/state_level/state_level_highway_weights.csv', index_col=0, dtype={'state_fips':str})\n",
    "state_highwayweights.index = state_highwayweights.columns = [fips.zfill(2) for fips in state_highwayweights.columns.to_list()]\n",
    "state_highwayweights = state_highwayweights.reindex(index=state_fips_list,\n",
    "                                    columns=state_fips_list,\n",
    "                                    fill_value=0.0)"
   ],
   "id": "fa56e89c8bc80795",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:06.366979Z",
     "start_time": "2025-11-13T10:49:06.340341Z"
    }
   },
   "cell_type": "code",
   "source": "state_airweights.index.to_list() == state_borderweights.index.to_list() == state_highwayweights.index.to_list()",
   "id": "2b6a44dbaca248d7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:06.382602Z",
     "start_time": "2025-11-13T10:49:06.366979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "W1 = normalize_df(state_airweights)\n",
    "W2 = normalize_df(state_borderweights) \n",
    "W3 = normalize_df(state_highwayweights) \n",
    "A_state = W1 + W2 + W3"
   ],
   "id": "7c5ce36110e41747",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:06.429473Z",
     "start_time": "2025-11-13T10:49:06.382602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "v_out = len(variant_cols)\n",
    "target_variant_indices = [feature_cols.index(v) for v in variant_cols]"
   ],
   "id": "8862aa0fb3f72720",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train",
   "id": "f034ea870e5f80e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:06.467312Z",
     "start_time": "2025-11-13T10:49:06.430309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_len = 14\n",
    "horizon = 7\n",
    "\n",
    "dataset = msst_classes.EpidemicDataset(\n",
    "    X_state=X_state,\n",
    "    X_county=X_county,\n",
    "    input_len=input_len,\n",
    "    horizon=horizon,\n",
    "    target_variant_indices=target_variant_indices,\n",
    ")\n",
    "\n",
    "N = len(dataset)\n",
    "train_ratio = 0.8\n",
    "N_train = int(N * train_ratio)\n",
    "train_indices = np.arange(0, N_train)\n",
    "val_indices = np.arange(N_train, N)\n",
    "\n",
    "# tiny_indices = np.arange(0, 10)   # or even 0..4\n",
    "# train_ds = Subset(dataset, tiny_indices)\n",
    "# train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "train_ds = Subset(dataset, train_indices)\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "val_ds   = Subset(dataset, val_indices)\n",
    "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False)\n"
   ],
   "id": "18907e5b04d7e77a",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:49:06.482385Z",
     "start_time": "2025-11-13T10:49:06.469313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "hidden_gcn = 64\n",
    "hidden_gru = 64\n",
    "v_out = len(target_variant_indices)\n",
    "\n",
    "# model = msst_classes.MSSTVariant(\n",
    "#     macro_in=F_macro,\n",
    "#     micro_in=F_micro,\n",
    "#     hidden_gcn=hidden_gcn,\n",
    "#     hidden_gru=hidden_gru,\n",
    "#     horizon=horizon,\n",
    "#     v_out=v_out,\n",
    "#     A_state=A_state,\n",
    "#     A_county_global=A_county_global,\n",
    "#     state_of_county=state_of_county,\n",
    "# ).to(device)\n",
    "\n",
    "model = msst_classes.GRUOnly(F_macro, hidden_gru=32, horizon=horizon, v_out=v_out).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "loss_fn = nn.MSELoss()\n"
   ],
   "id": "e2a21c15a634f98e",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:50:59.045009Z",
     "start_time": "2025-11-13T10:49:06.484384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for X_state_seq, X_county_seq, y_true in train_loader:\n",
    "\n",
    "        # Remove batch dimension (batch_size=1)\n",
    "        X_state_seq = X_state_seq.squeeze(0).to(device)    # (T_in, S, F_macro)\n",
    "        X_county_seq = X_county_seq.squeeze(0).to(device)  # (T_in, M, F_micro)\n",
    "        y_true = y_true.squeeze(0).to(device)              # (S, horizon, v_out)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_state_seq, X_county_seq)          # (S, horizon, v_out)\n",
    "\n",
    "        loss = loss_fn(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        # print(\"RMSE loss / y_mean:\", np.sqrt(loss.item()) / y_true.mean())\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for X_state_seq, X_county_seq, y_true in val_loader:\n",
    "            X_state_seq = X_state_seq.squeeze(0).to(device)\n",
    "            X_county_seq = X_county_seq.squeeze(0).to(device)\n",
    "            y_true = y_true.squeeze(0).to(device)\n",
    "\n",
    "            y_pred = model(X_state_seq, X_county_seq)\n",
    "            loss = loss_fn(y_pred, y_true)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    mean_train = np.mean(train_losses) if train_losses else float(\"nan\")\n",
    "    mean_val = np.mean(val_losses) if val_losses else float(\"nan\")\n",
    "    print(f\"Epoch {epoch:03d}: train_loss={mean_train:.4f}, val_loss={mean_val:.4f}\")\n"
   ],
   "id": "578aa1b8d27828fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: train_loss=2.7115, val_loss=0.0367\n",
      "Epoch 002: train_loss=0.9729, val_loss=0.0337\n",
      "Epoch 003: train_loss=0.4818, val_loss=0.0332\n",
      "Epoch 004: train_loss=0.3327, val_loss=0.0457\n",
      "Epoch 005: train_loss=0.2920, val_loss=0.0321\n",
      "Epoch 006: train_loss=0.2392, val_loss=0.0374\n",
      "Epoch 007: train_loss=0.2369, val_loss=0.0436\n",
      "Epoch 008: train_loss=0.2251, val_loss=0.0324\n",
      "Epoch 009: train_loss=0.2106, val_loss=0.0402\n",
      "Epoch 010: train_loss=0.1957, val_loss=0.0545\n",
      "Epoch 011: train_loss=0.2062, val_loss=0.0344\n",
      "Epoch 012: train_loss=0.2004, val_loss=0.0322\n",
      "Epoch 013: train_loss=0.1764, val_loss=0.0289\n",
      "Epoch 014: train_loss=0.1804, val_loss=0.0318\n",
      "Epoch 015: train_loss=0.1644, val_loss=0.0371\n",
      "Epoch 016: train_loss=0.1800, val_loss=0.0336\n",
      "Epoch 017: train_loss=0.1602, val_loss=0.0296\n",
      "Epoch 018: train_loss=0.1661, val_loss=0.0278\n",
      "Epoch 019: train_loss=0.1498, val_loss=0.0322\n",
      "Epoch 020: train_loss=0.1446, val_loss=0.0298\n",
      "Epoch 021: train_loss=0.1405, val_loss=0.0299\n",
      "Epoch 022: train_loss=0.1479, val_loss=0.0301\n",
      "Epoch 023: train_loss=0.1306, val_loss=0.0282\n",
      "Epoch 024: train_loss=0.1401, val_loss=0.0294\n",
      "Epoch 025: train_loss=0.1505, val_loss=0.0317\n",
      "Epoch 026: train_loss=0.1264, val_loss=0.0273\n",
      "Epoch 027: train_loss=0.1269, val_loss=0.0291\n",
      "Epoch 028: train_loss=0.1204, val_loss=0.0298\n",
      "Epoch 029: train_loss=0.1212, val_loss=0.0436\n",
      "Epoch 030: train_loss=0.1457, val_loss=0.0303\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Eval per State",
   "id": "768f1d80f89d22cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:50:59.065190Z",
     "start_time": "2025-11-13T10:50:59.047025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_per_state_rmse(model, loader, device, case_scale=1.0):\n",
    "    \"\"\"\n",
    "    Returns: rmse_per_state: np.ndarray of shape (S,)\n",
    "    RMSE is computed over *all* windows, horizons, and output variants.\n",
    "    If you trained on scaled cases, pass case_scale to get RMSE in original units.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    sum_sq = None   # will become tensor of shape (S,)\n",
    "    count = 0       # total number of (horizon * v_out * num_windows)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_state_seq, X_county_seq, y_true in loader:\n",
    "            # remove batch dimension (batch_size=1)\n",
    "            X_state_seq = X_state_seq.squeeze(0).to(device)    # (T_in, S, F_macro)\n",
    "            X_county_seq = X_county_seq.squeeze(0).to(device)  # (T_in, M, F_micro)\n",
    "            y_true = y_true.squeeze(0).to(device)              # (S, H, v_out)\n",
    "\n",
    "            y_pred = model(X_state_seq, X_county_seq)          # (S, H, v_out)\n",
    "\n",
    "            # Undo scaling if needed so RMSE is in original case units\n",
    "            diff = (y_pred - y_true) * case_scale              # (S, H, v_out)\n",
    "\n",
    "            se = diff ** 2                                     # (S, H, v_out)\n",
    "            # sum over horizon + variant dimensions -> per-state sum squared error\n",
    "            se_per_state = se.sum(dim=(1, 2))                  # (S,)\n",
    "\n",
    "            if sum_sq is None:\n",
    "                sum_sq = se_per_state\n",
    "            else:\n",
    "                sum_sq += se_per_state\n",
    "\n",
    "            # each window contributes H * v_out elements per state\n",
    "            H = y_pred.shape[1]\n",
    "            V = y_pred.shape[2]\n",
    "            count += H * V\n",
    "\n",
    "    mse_per_state = sum_sq / count                             # (S,)\n",
    "    rmse_per_state = torch.sqrt(mse_per_state)                 # (S,)\n",
    "\n",
    "    return rmse_per_state.cpu().numpy()\n"
   ],
   "id": "89845592120eff14",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baselines",
   "id": "fc1e755c9a9547f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:53:23.622364Z",
     "start_time": "2025-11-13T10:53:23.588206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_last_value_baseline(loader, device, target_variant_indices, case_scale=1.0):\n",
    "    \"\"\"\n",
    "    Baseline: y_pred(s, h, v) = last observed value in the input window for that state & variant.\n",
    "    Returns: rmse_per_state: np.ndarray of shape (S,)\n",
    "    \"\"\"\n",
    "    sum_sq = None\n",
    "    count = 0\n",
    "    target_variant_indices = torch.as_tensor(target_variant_indices, dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_state_seq, X_county_seq, y_true in loader:\n",
    "            # squeeze batch dim (batch_size = 1)\n",
    "            X_state_seq = X_state_seq.squeeze(0).to(device)    # (T_in, S, F_macro)\n",
    "            y_true = y_true.squeeze(0).to(device)              # (S, H, v_out)\n",
    "\n",
    "            T_in, S, F_macro = X_state_seq.shape\n",
    "            H = y_true.shape[1]\n",
    "            V = y_true.shape[2]\n",
    "\n",
    "            # last time step: (S, F_macro)\n",
    "            last_state = X_state_seq[-1]                       # (S, F_macro)\n",
    "\n",
    "            # pick only target variants: (S, v_out)\n",
    "            last_targets = last_state[:, target_variant_indices]   # (S, v_out)\n",
    "\n",
    "            # repeat across horizon: (S, H, v_out)\n",
    "            y_pred = last_targets.unsqueeze(1).expand(S, H, V)\n",
    "\n",
    "            # undo scaling if needed\n",
    "            diff = (y_pred - y_true) * case_scale              # (S, H, v_out)\n",
    "            se = diff ** 2\n",
    "            se_per_state = se.sum(dim=(1, 2))                  # (S,)\n",
    "\n",
    "            if sum_sq is None:\n",
    "                sum_sq = se_per_state\n",
    "            else:\n",
    "                sum_sq += se_per_state\n",
    "\n",
    "            count += H * V\n",
    "\n",
    "    mse_per_state = sum_sq / count\n",
    "    rmse_per_state = torch.sqrt(mse_per_state)\n",
    "    return rmse_per_state.cpu().numpy()\n",
    "\n",
    "\n",
    "def evaluate_moving_average_baseline(loader, device, target_variant_indices, case_scale=1.0, K=7):\n",
    "    \"\"\"\n",
    "    Baseline: y_pred(s, h, v) = mean of last K days in the input window (per state & variant).\n",
    "    Returns: rmse_per_state: np.ndarray of shape (S,)\n",
    "    \"\"\"\n",
    "    sum_sq = None\n",
    "    count = 0\n",
    "    target_variant_indices = torch.as_tensor(target_variant_indices, dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_state_seq, X_county_seq, y_true in loader:\n",
    "            X_state_seq = X_state_seq.squeeze(0).to(device)    # (T_in, S, F_macro)\n",
    "            y_true = y_true.squeeze(0).to(device)              # (S, H, v_out)\n",
    "\n",
    "            T_in, S, F_macro = X_state_seq.shape\n",
    "            H = y_true.shape[1]\n",
    "            V = y_true.shape[2]\n",
    "\n",
    "            k = min(K, T_in)\n",
    "            # last k days: (k, S, F_macro)\n",
    "            last_k = X_state_seq[-k:]                          # (k, S, F_macro)\n",
    "\n",
    "            # mean over time: (S, F_macro)\n",
    "            mean_state = last_k.mean(dim=0)\n",
    "\n",
    "            # pick only target variants: (S, v_out)\n",
    "            mean_targets = mean_state[:, target_variant_indices]  # (S, v_out)\n",
    "\n",
    "            # repeat across horizon: (S, H, v_out)\n",
    "            y_pred = mean_targets.unsqueeze(1).expand(S, H, V)\n",
    "\n",
    "            diff = (y_pred - y_true) * case_scale              # (S, H, v_out)\n",
    "            se = diff ** 2\n",
    "            se_per_state = se.sum(dim=(1, 2))                  # (S,)\n",
    "\n",
    "            if sum_sq is None:\n",
    "                sum_sq = se_per_state\n",
    "            else:\n",
    "                sum_sq += se_per_state\n",
    "\n",
    "            count += H * V\n",
    "\n",
    "    mse_per_state = sum_sq / count\n",
    "    rmse_per_state = torch.sqrt(mse_per_state)\n",
    "    return rmse_per_state.cpu().numpy()\n",
    "\n",
    "\n",
    "def rmse_by_horizon_model(model, loader, device, case_scale=1.0):\n",
    "    model.eval()\n",
    "    sum_sq = None  # (H,)\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for X_state_seq, X_county_seq, y_true in loader:\n",
    "            X_state_seq = X_state_seq.squeeze(0).to(device)   # (T_in, S, F_macro)\n",
    "            y_true = y_true.squeeze(0).to(device)             # (S, H, V)\n",
    "            y_pred = model(X_state_seq, X_county_seq.squeeze(0).to(device)\n",
    "                           if X_county_seq is not None else None)\n",
    "\n",
    "            diff = (y_pred - y_true) * case_scale             # (S, H, V)\n",
    "            se = diff ** 2                                    # (S, H, V)\n",
    "            se_h = se.sum(dim=(0, 2))                         # (H,)\n",
    "\n",
    "            if sum_sq is None:\n",
    "                sum_sq = se_h\n",
    "            else:\n",
    "                sum_sq += se_h\n",
    "\n",
    "            S = y_true.shape[0]\n",
    "            V = y_true.shape[2]\n",
    "            count += S * V\n",
    "\n",
    "    mse_h = sum_sq / count                                    # (H,)\n",
    "    return torch.sqrt(mse_h).cpu().numpy()                    # (H,)\n",
    "\n",
    "def rmse_by_horizon_last(loader, device, target_variant_indices, case_scale=1.0, K=7):\n",
    "    target_variant_indices = torch.as_tensor(target_variant_indices, dtype=torch.long, device=device)\n",
    "    sum_sq = None\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for X_state_seq, X_county_seq, y_true in loader:\n",
    "            X_state_seq = X_state_seq.squeeze(0).to(device)   # (T_in, S, F_macro)\n",
    "            y_true = y_true.squeeze(0).to(device)             # (S, H, V)\n",
    "\n",
    "            T_in, S, F_macro = X_state_seq.shape\n",
    "            H = y_true.shape[1]\n",
    "            V = y_true.shape[2]\n",
    "\n",
    "            last_state = X_state_seq[-1]                      # (S, F_macro)\n",
    "            last_targets = last_state[:, target_variant_indices]  # (S, V)\n",
    "            y_pred = last_targets.unsqueeze(1).expand(S, H, V)\n",
    "\n",
    "            diff = (y_pred - y_true) * case_scale\n",
    "            se = diff ** 2\n",
    "            se_h = se.sum(dim=(0, 2))                         # (H,)\n",
    "\n",
    "            if sum_sq is None:\n",
    "                sum_sq = se_h\n",
    "            else:\n",
    "                sum_sq += se_h\n",
    "\n",
    "            count += S * V\n",
    "\n",
    "    mse_h = sum_sq / count\n",
    "    return torch.sqrt(mse_h).cpu().numpy()\n"
   ],
   "id": "da794eed63f78b49",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:50:59.420440Z",
     "start_time": "2025-11-13T10:50:59.097364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "case_scale = 1000.0  # or 1.0 if you didn't scale cases\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# Model RMSE\n",
    "rmse_model = evaluate_per_state_rmse(model, val_loader, device, case_scale=case_scale)\n",
    "\n",
    "# Baselines\n",
    "rmse_last = evaluate_last_value_baseline(val_loader, device, target_variant_indices, case_scale=case_scale)\n",
    "rmse_ma   = evaluate_moving_average_baseline(val_loader, device, target_variant_indices, case_scale=case_scale, K=7)\n",
    "\n",
    "print(\"Overall mean RMSE (model):      \", rmse_model.mean())\n",
    "print(\"Overall mean RMSE (last value):\", rmse_last.mean())\n",
    "print(\"Overall mean RMSE (7-day MA):  \", rmse_ma.mean())\n",
    "\n",
    "print(\"\\nPer-state comparison (first few):\")\n",
    "for s_idx, s_fips in enumerate(state_fips_list[:10]):  # or all of them\n",
    "    print(\n",
    "        f\"State {s_fips}: \"\n",
    "        f\"model={rmse_model[s_idx]:8.1f}, \"\n",
    "        f\"last={rmse_last[s_idx]:8.1f}, \"\n",
    "        f\"ma={rmse_ma[s_idx]:8.1f}\"\n",
    "    )\n"
   ],
   "id": "a0ac01a7b02fa5f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean RMSE (model):       113.54932\n",
      "Overall mean RMSE (last value): 98.323654\n",
      "Overall mean RMSE (7-day MA):   107.30355\n",
      "\n",
      "Per-state comparison (first few):\n",
      "State 01: model=   168.4, last=   165.9, ma=   157.5\n",
      "State 04: model=   102.9, last=    95.4, ma=   116.8\n",
      "State 05: model=    36.7, last=    24.2, ma=    34.1\n",
      "State 06: model=   409.3, last=   419.9, ma=   525.9\n",
      "State 08: model=    60.5, last=    52.9, ma=    58.7\n",
      "State 09: model=    53.9, last=    40.8, ma=    42.5\n",
      "State 10: model=    31.2, last=    10.3, ma=    11.3\n",
      "State 11: model=    28.7, last=     8.6, ma=     8.7\n",
      "State 12: model=   793.8, last=   743.0, ma=   743.9\n",
      "State 13: model=   114.2, last=   113.7, ma=   132.4\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:53:25.857435Z",
     "start_time": "2025-11-13T10:53:25.512111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rmse_h_model = rmse_by_horizon_model(model, val_loader, device, case_scale)\n",
    "rmse_h_last  = rmse_by_horizon_last(val_loader, device, target_variant_indices, case_scale)\n",
    "\n",
    "for d in range(len(rmse_h_model)):\n",
    "    print(f\"Day {d+1}: model={rmse_h_model[d]:.2f}, last={rmse_h_last[d]:.2f}\")\n"
   ],
   "id": "8dbb6379a3130496",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1: model=111.76, last=90.93\n",
      "Day 2: model=133.15, last=116.73\n",
      "Day 3: model=152.03, last=137.73\n",
      "Day 4: model=169.90, last=156.79\n",
      "Day 5: model=190.17, last=175.51\n",
      "Day 6: model=208.59, last=192.67\n",
      "Day 7: model=224.34, last=210.79\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e4c34c1d2e4ff6de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
